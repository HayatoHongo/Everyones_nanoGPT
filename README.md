It is based on Andrej Karpathy’s nano-GPT. The original source is free license and allows commercial use.

https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing

It uses a bigram model, where each token is just one character. Also, the internal structure is very simple.  
The training text dataset is Shakespeare’s writings. Since they are very old, they are copyright-free.

Compared to the real GPT-2, it’s quite basic. But start here to master the GPT basics.  
In just 2-4 minutes of CPU training (on a 16GB memory PC), you can generate Shakespeare-like text. It’s guaranteed to impress!
